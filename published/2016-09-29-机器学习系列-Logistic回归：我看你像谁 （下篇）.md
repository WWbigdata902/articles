#机器学习系列-Logistic回归：我看你像谁 （下篇）

###作者：向日葵

##Logistic回归

书接上回，在我们有了最小二乘法与极大似然估计做基础之后，这样我们就做好了Logistic回归的准备，渐渐的进入到我们的主题Logistic回归。
很多都属于分类的问题了，邮件（垃圾邮件/非垃圾邮件），肿瘤（良性/恶性）。二分类问题，可以用如下形式来定义它：
y∈{0,1},其中0属于负例，1属于正例。
现在来构造一种状态，一个向量来代表肿瘤（良性/恶性）和肿瘤大小的关系。
![](http://static.datartisan.com/upload/attachment/2016/08/3lfnBvxK.png)  

![](http://static.datartisan.com/upload/attachment/2016/08/mQXq2ocW.png)  

Sigmoid 函数在有个很漂亮的“S"形，如下图所示（引自维基百科）：

![](http://static.datartisan.com/upload/attachment/2016/08/TsbgunHo.png)

综合上述两式，我们得到逻辑回归模型的数学表达式：

![](http://static.datartisan.com/upload/attachment/2016/08/ZNA1a2Nv.png)  

Cost函数和J函数如下，它们是基于最大似然估计推导得到的。  

![](http://static.datartisan.com/upload/attachment/2016/08/Ym6n38MT.png)  

下面详细说明推导的过程：  
![](http://static.datartisan.com/upload/attachment/2016/08/XC0ftXPF.png)  

最大似然估计就是求使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为下式，即：  

![](http://static.datartisan.com/upload/attachment/2016/08/FxE8zpql.png)  

梯度下降法求的最小值
![](http://static.datartisan.com/upload/attachment/2016/08/NhIMCP8I.png)

![](http://static.datartisan.com/upload/attachment/2016/08/uOphHTd5.png)

####向量化Vectorization  
Vectorization是使用矩阵计算来代替for循环，以简化计算过程，提高效率。
如上式，Σ(...)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization。  

下面介绍向量化的过程：
约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值：
![](http://static.datartisan.com/upload/attachment/2016/08/i4x8ot7n.png)  
![](http://static.datartisan.com/upload/attachment/2016/08/36wY9jrm.png)  

Logistic回归的推导过程，采用的是极大似然法和梯度下降法取得各个参数的迭代过程。以后很多公式的推导也是类似这个过程，机器学习的过程大部分的算法都归结到概率论，如果概率论不是很熟，可以继续温习一下。所以很多人都在总觉，机器学习的问题，归宗到底就是概率论的问题。而采用极大似然的算法，其中隐藏着一个道理：求出来的参数会是最符合我们观察到的结果，实验数据决定了我们的参数。  

###TensorFlow下的Logistic回归
现在有大量的机器学习的框架，个人开发者，大公司等都有。比较出名的还是FaceBook和谷歌的开源框架。  

TensorFlow是谷歌2015年开源的学习框架，结合了大量的机器学习的算法，官方的文档也比较清楚，开篇的初学者入门讲的就是关于Logistic回归的问题，这里简单的介绍一下，主要是想说明TensorFlow还是属于比较强大的工具，可以进行工具的学习。  

这篇文档的主要介绍如何使用TensorFlow识别MNIST，关于MNIST在之前神经网络的介绍有介绍过。MNIST里存放着一些手写的数据：  

![](http://static.datartisan.com/upload/attachment/2016/08/Gm4CxaAD.png)  

每个数字都可以用二进制向量数组来表示：  

![](http://static.datartisan.com/upload/attachment/2016/08/LaEy7u9O.png)

这些数据为神经网络的输入：  
![](http://static.datartisan.com/upload/attachment/2016/08/mbuGXUDk.png)  
利用Logistic回归的训练求解上面的参数。
代码在https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py下可以自己参看。  

###总结

这个章节里介绍了Logistic回归和推导的这个过程，Logistic回归是机器学习里最经常用到的算法，也是最基础的算法，通过推导Logistic回归就能够清楚机器学习的基础知识，后面有些算法的思想也和Logistic回归算法类似。

***
![](http://static.datartisan.com/upload/attachment/2016/05/xKM5xlV4.png)


 


