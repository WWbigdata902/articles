## 机器学习系列-word2vec篇 ##

###开篇 

深度学习方向当下如火如荼，就差跑进楼下大妈的聊天内容了。深度学习的宝藏很多，一个小领域的一段小代码，都可以发出璀璨的光芒。如果你也刚刚踏入这方向，一开始难免有一些彷徨，但慢慢会有，清晨入古寺 初日照高林，那种博大的体验。

word2vec就是这样的一小段代码，如果你对word2vec的代码了如指掌，那你可以直接return。这是一篇关于word2vec介绍的文章，读完以后你会欣喜的发现自己会灵活的使用word2vec，但你也可能会郁闷，因为还是会觉得像是盲人摸象一样，完全对深度学习没有一点头绪。没关系，谁不是这样一点一滴的积累起来的呢。

###从需求入门

美国大选刚刚落幕，川普胜出。假设反过来想，给你一个川普的词，你会联想到哪些？正常的话，应该是美国、大选、希拉里、奥巴马；也就是相似词语的选取了。对于相识词的选取，算法非常的多。也有许多人用了很简单的办法就能求得两样东西的相似性，比如购物车里物品的相似度，最简单的办法就是看看同时买了这样东西的用户还同时买了什么，用简单的数据结构就很容易实现这样的一个算法。这种算法很简单也很方便，但就是这种简单而使他忽略了很多的问题。例如时间顺序，下面会有提到。

还是回归到相识度的问题。归结到数学问题上，最经常用的是把每个词都归结到一个坐标系下，再用距离公式（如：皮尔逊公式）可方便的求出各个词语之间的相识度。

这也是word2vec的方法，word2vec 通过训练，可以把对文本内容的处理简化为 K 维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。
如图，下面是有五维向量空间的单词：
![](http://chuantu.biz/t5/41/1478758660x2102250234.png)

算法的关键步骤就是如何求出词语的向量空间。


###word2vec算法介绍


word2vec是2013年Google中开源的一款工具。2013年神经网络的各种算法都已经相当的成熟了，word2vec核心是神经网络的方法，采用 CBOW（Continuous Bag-Of-Words，即连续的词袋模型）和 Skip-Gram 两种模型，将词语映像到同一坐标系，得出数值向量的高效工具。

一般来说算法采用神经网络的话，要注意他的输入和输出。因为使用神经网络进行训练需要有输入和输出，输入通过神经网络后，通过和输入对比，进行神经网络的重新调整，达到训练网络的目的。抓住输入输出就能够很好的理解神经网络的算法过程。

语言模型采用神经网络，就要判断什么东西要作为输入，什么东西要作为输出。这是算法可以创新的地方，语言模型有许多种，大部分的原理也是采用根据上下文，来推测这个词的概率。

word2vec输入输出也算是鬼斧神功，算法跟哈夫曼树有关系。哈夫曼树可以比较准确的表达这边文章的结构。

a,b,c,d分别表示不同词，并附加找个词出现的频率，这些词就能有自己的路径和编码。

![](http://chuantu.biz/t5/41/1478765121x2102250234.png)

关于哈夫曼树就不仔细详细说明了，他是一种压缩算法，能很好的保持一篇文章的特性。

训练的过程是，把每一段落取出来，每个词都通过哈夫曼树对应的路径和编码。编码是(0和1)，作为神经网络的输出，每个路径初始化一个给定维数的向量，跟自己段落中的每个词作为输入，进行反向的迭代，就可以训练出参数。

这就是算法的整个过程。

###快速入门

1. 代码下载： http://word2vec.googlecode.com/svn/trunk/ 
1. 针对个人需求修改 makefile 文件，比如作者使用的 linux 系统就需要把 makefile 编译选项中的-Ofast 要更改为-O2 或者-g（调试时用）， 同时删除编译器无法辨认的-march=native 和-Wno-unused-result 选项。 有些系统可能还需要修改相关的 c 语言头文件，具体网上搜搜应该可以解决。 
1. 运行“ make”编译 word2vec 工具。 

1. 运行 demo 脚本： ./demo-word.sh

 demo-word.sh主要工作为： 
- 
- 1）编译（ make） 
- 2）下载训练数据 text8，如果不存在。 text8 中为一些空格隔开的英文单词，但不含标点符号，一共有 1600 多万个单词。 
- 3）训练，大概一个小时左右，取决于机器配置 
- 4）调用distance，查找最近的词

python版本的命令如下：

> Python的命令为python word2vec.py -train tx -model vb -cbow 0  -negative 0 -dim 5


###应用

word2vec是根据文章中每个词的上下关系，把每个词的关系映射到同一坐标系下，构成了一个大矩阵，矩阵下反映了每个词的关系。这些词的关系是通过上下文相关得出来的，它具有前后序列性，而Word2vec同时采用了哈夫曼的压缩算法，对是一些热门词进行了很好的降权处理。因此他在做一些相似词，或者词语的扩展都有很好的效果。

这种相识性还可以用在，物品的推荐上，根据用户购买物品的顺序，把每个物品当成一个单词，相当于一门外语了，谁也看不懂而已，但里面放映了上下文的关系，这个是很重要的，也是我们一开头那种普通算法无法做到的，同时对一些热门的物品自然有降权的处理，非常的方便。

word2vec自然规避了两大问题：词语的次序和热门词语的降权处理。
