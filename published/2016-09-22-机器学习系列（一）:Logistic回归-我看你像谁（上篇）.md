**作者：**向日葵

##引子：
依旧是一个“烧烤”的天气，我在的城市厦门，已经达到了37度的高温了，不过也好，只有这样的暑假，这样的天气才有时间和心思，静下心来思考一些东西。  

关于机器学习的教程确实是太多了，处于这种变革的时代，出去不说点机器学习的东西，都觉得自己落伍了，但总觉得网上的东西并不系统，无法让人串联在一起，总有很多人读了几篇机器学习的东西，就自以为机器学习就那些东西，认为机器学习也就那么一回事，想把这几年关于机器学习的东西做一些总结，能够跟大家一起学习和交流。  

如果需要用几句话来简单的总结机器学习是什么意思，也许可以用：让机器学会决策。对比我们人来说，每天都会碰到这个问题，比如菜市场里买芒果，总要挑出哪些是甜的。这就是所谓的决策，再通俗来讲就是分类问题了，把一堆芒果，分出甜和不甜的。而机器学习就是学会把甜和不甜的芒果分出来，那如何分呢？模拟人类的思考方式。凭经验，我们可以按照芒果皮的颜色，大小等来对芒果的酸甜进行分类，对于机器来说，把芒果的颜色，大小等当成变量输入到电脑模型里，就能推出芒果的酸甜性，这样就对芒果进行分类。  

机器学习的分类算法有非常的多，这篇主要介绍的是Logistic回归。



##最小二乘法和极大似然估计
从一个最简单的数学问题开始。  

1801年，意大利天文学家朱赛普·皮亚齐发现了第一颗小行星谷神星。经过40天的跟踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。时年24岁的高斯也计算了谷神星的轨道。奥地利天文学家海因里希·奥尔伯斯根据高斯计算出来的轨道重新发现了谷神星。    

从这段历史记录可以看出，高斯当时观察了很多小行星谷神星的记录，也就是我们常说的观察数据，并使用了最小二乘法模拟了这条线，预测了小行星谷神星的轨迹。  

高斯最小二乘法的方法发表于1809年他的著作《天体运动论》中。其实在1806年法国科学家勒让德就提出了最小二乘法相应的想法，所以勒让德曾与高斯为谁最早创立最小二乘法原理发生争执。  

最小二乘法的思想是什么呢？

![chart-1](http://static.datartisan.com/upload/attachment/2016/08/MflWyCAz.png)

假设观察数据(x_1,y_1 ), (x_2,y_2 ), (x_3,y_3 )…,(x_n,y_n ),而默认这些数据是符合最常见的规律，既x和y符合线性关系，用方程可以表示为
y=a+bx  

其中，a,b是我们需要通过观察数据确定的参数。

![chart-2](http://static.datartisan.com/upload/attachment/2016/08/NT3gDCSO.png) 

所谓最小二乘法就是这样的一个法则，按照这样法则，最好是拟合于各个数据点的最佳曲线应该使个数据点与曲线偏差的平均和为最小。  

用数学公式表示为：

![chart-3](http://static.datartisan.com/upload/attachment/2016/08/qeBubq0U.png)

其公式的值为最小，这里的a,b是参数。有两个参数，求最小值，即为求偏差平方和对a和b分别求出偏导数，得：  
![chart-4](http://static.datartisan.com/upload/attachment/2016/08/SBL5Yzdy.png)

根据公式可以推出a和b的值：  

![chart-5](http://static.datartisan.com/upload/attachment/2016/08/g5JfWKEv.png)  

这样就可以求出了a和b的值。  
  
既我们可以通过观察的数据，来拟合我们的直线，既可以在给定某个x，有效的预测y。通常来说求出的值a和b跟实际本身来说是有一定的误差了，是不是给定的观察值越多就越准确呢？这不一定，这也是大学概率论和数值统计中一直讨论的问题。  
  
再次的强调最小二乘法是使用误差最小来进行估计的。    

回头过来看，可能会觉得最小二乘法跟我们讨论中的芒果酸甜问题，并不是一回事。但从另外一种退化的角度来讲：  

在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。这时，输入变量X可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器(classifier)。分类器对新的输入进行输出的预测(prediction)，称为分类(classification)。  

继续一个简单的故事：    
某位同学与一位猎人一起出去打猎，一只兔子从前方窜过。只听见一声枪响，野兔应声倒下，如果要你来推测，这一发命中的子弹是谁打的？你会怎么想呢？正常的情况下，猎人的枪法肯定比你的同学的枪法好，也就是说猎人的命中率比你的同学高。  而一枪就打死兔子，命中率是100%的，这么高的命中率，应该是谁打中的呢？显然，猎人开的枪比较符合我们观察的想象了吧。如果是开了三枪才打中兔子的话，那枪法就不怎么样了，某同学开的枪比较符合已经发生的现象了。   

这就是要讲的，极大似然法。  
如果试验n次，我们得到n个样本，极大似然估计是要是所求的概率，最大限制的符合我们现在所发生的。  
这里我们这样定义似然函数：  
假设{y_1,…,y_n }为独立同分布，则样本数据的联合密度函数为  f(y_1;θ)f(y_2;θ)∙∙∙f(y_n;θ)，定义“似然函数”为，  

![chart-6](http://static.datartisan.com/upload/attachment/2016/08/01ZTevVL.png)   

为了较好的说明，举一个很简单的例子：两点分布的情况。  
设某工序生产的产品不合格率为p，抽n个产品作检验，发现有T个不合格，试求p的极大似然估计值。  
在这里我们做了n次的试验，我们所求的概率p要符合我们试验的结果，也就是通过极大似然函数来求解。  

![chart-7](http://static.datartisan.com/upload/attachment/2016/08/L2unY7KX.png)  

最大似然函数的思想也就是想使我们求得的概率符合我们所观察的。而最大似然法看起来，好像只是为了求得某个概率，但恰恰是我们Logistic回归中用到的一种方法。
了解了最小二乘法与极大似然估计之后，我们暂时先休息一下，在下半部分我们会进入正题介绍 Logistic 回归，以及简单介绍 Logistic 在 TensorFlow 中的用法。

***
 ![Company-LOGO](http://static.datartisan.com/upload/attachment/2016/05/xKM5xlV4.png) 
  







































